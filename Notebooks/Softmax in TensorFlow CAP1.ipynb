{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía:**\n",
    "Practical Convolutional Neural Networks: Implement advanced deep learning models using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**\n",
    "\n",
    "La función softmax convierte sus entradas, conocidas como logit o logit scores, entre 0 y 1, y también normaliza las salidas para que todas sumen 1. En otras palabras, la función softmax convierte sus logits en probabilidades. Matemáticamente, la función softmax se define de la siguiente manera:\n",
    "\n",
    "![Esta es una imagen de ejemplo](https://www.oreilly.com/library/view/practical-convolutional-neural/9781788392303/assets/f397aff3-9485-43c6-ae4f-7a33d9374b1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En TensorFlow, se implementa la función softmax. Toma logits y devuelve activaciones softmax que tienen el mismo tipo y forma que los logits de entrada, como se muestra en la siguiente imagen:\n",
    "\n",
    "![](https://miro.medium.com/max/1058/0*2r10e7gw1jzOsHhC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\anaconda3.1\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45659032 0.3382504  0.20515925]\n"
     ]
    }
   ],
   "source": [
    "## The following code is used to implement this:\n",
    "logit_data = [1.2, 0.9, 0.4]\n",
    "logits = tf.placeholder(tf.float32)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "    print( output )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma en que representamos las etiquetas matemáticamente a menudo se denomina codificación one-hot. Cada etiqueta está representada por un vector que tiene 1.0 para la etiqueta correcta y 0.0 para todo lo demás. Esto funciona bien para la mayoría de los casos problemáticos. Sin embargo, cuando el problema tiene millones de etiquetas, la codificación one-hot no es eficiente, ya que la mayoría de los elementos vectoriales son ceros. Medimos la distancia de similitud entre dos vectores de probabilidad, conocida como entropía cruzada y denotada por D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el aprendizaje automático, definimos lo que significa que un modelo sea malo generalmente mediante una función matemática. Esta función se llama función de pérdida, costo o objetivo. Una función muy común que se utiliza para determinar la pérdida de un modelo se llama pérdida de entropía cruzada. Este concepto proviene de la teoría de la información (para obtener más información al respecto, consulte Teoría de la información visual en https://colah.github.io/posts/2015-09-Visual-Information/). Intuitivamente, la pérdida será alta si el modelo clasifica mal los datos de entrenamiento, y será baja en caso contrario, como se muestra aquí:\n",
    "\n",
    "![](cross_entropy.png \"cross entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En TensorFlow, podemos escribir una función de entropía cruzada usando tf.reduce_sum (); toma una matriz de números y devuelve su suma como un tensor (consulte el siguiente bloque de código):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[2 2 2]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1,1,1], [1,1,1]])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.reduce_sum([1,2,3]))) #returns 6\n",
    "    print(sess.run(tf.reduce_sum(x,0))) #sum along x axis, prints [2,2,2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero en la práctica, al calcular la función softmax, los términos intermedios pueden ser muy grandes debido a los exponenciales. Entonces, dividir números grandes puede ser numéricamente inestable. Deberíamos usar la API de pérdida de entropía cruzada y softmax proporcionada por TensorFlow. El siguiente fragmento de código calcula manualmente la pérdida de entropía cruzada y también imprime lo mismo con la API de TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\anaconda3.1\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0.6931472\n",
      "0.6931472\n"
     ]
    }
   ],
   "source": [
    "softmax_data = [0.1,0.5,0.4]\n",
    "onehot_data = [0.0,1.0,0.0]\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "onehot_encoding = tf.placeholder(tf.float32)\n",
    "cross_entropy = - tf.reduce_sum(tf.multiply(onehot_encoding,tf.log(softmax)))\n",
    "\n",
    "cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=tf.log(softmax),\n",
    "                                                             labels=onehot_encoding)\n",
    "with tf.Session() as session:\n",
    "    print(session.run(cross_entropy,feed_dict={softmax:softmax_data,\n",
    "                                               onehot_encoding:onehot_data} ))\n",
    "    print(session.run(cross_entropy_loss,feed_dict={softmax:softmax_data,\n",
    "                                                    onehot_encoding:onehot_data} ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
