{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eK8eUOQU7q2V"
   },
   "source": [
    "#  Redes neuronales recurrentes\n",
    "[**Python Deep Learning** Introducción práctica con Keras y TensorFlow 2. Jordi Torres. Editorial Marcombo ISBN: 9788426728289 ](https://www.marcombo.com/python-deep-learning-9788426728289/)\n",
    "\n",
    "[**Redes neuronales recurrentes** Jordi Torres](https://torres.ai/redes-neuronales-recurrentes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso de estudio: generacion de texto\n",
    "\n",
    "Este caso de estudio trata de generar texto usando una RNN basada en caracteres. Se entrena un modelo de red neuronal para predecir el siguiente caracter a partir de una secuencia de caracteres. Con este modelo intencionadamente simple, se consigue generar secuencias de texto mas largas llamando al modelo repetivamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Importar TensorFlow 2.0  y otras librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zWrJnG1jZd00",
    "outputId": "5dd18238-f276-437c-ac84-9b72b5fbbf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "#tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Descarga del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "qEt4CQ0ru7O5",
    "outputId": "a25fcbf9-4b76-4ed7-b654-17963146d458"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from google.colab import files\n",
    "#se debe cargar el fichero “Libro-Deep-Learning-introduccion-practica-con-Keras-1a-parte.txt”\n",
    "#files.upload()\n",
    "\n",
    "#path_to_fileDL ='/content/Libro-Deep-Learning-introduccion-practica-con-Keras-1a-parte.txt'\n",
    "\n",
    "path_to_fileDL = tf.keras.utils.get_file('Shakespear.txt', 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pD_55cOxLkAb",
    "outputId": "37cd06ca-04d1-484b-c5a9-a232bffe319c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto:        99993 carácteres\n",
      "El texto está compuesto de estos 62 carácteres:\n",
      "['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = open(path_to_fileDL, 'rb').read().decode(encoding='utf-8')\n",
    "print('Longitud del texto:        {} carácteres'.format(len(text)))\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print ('El texto está compuesto de estos {} carácteres:'.format(len(vocab)))\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esta tratando el caso de estudio a nivel de caracter, podriamos considerar que aqui el corpus son los caracteres, por tanto un corpus muy pequeño.\n",
    "\n",
    "Las redes neuronales solo procesan valores numericos, no letras, por tanto tenemos que traducir los caracteres a representacion numerica. Para ello se crean dos tablas de traduccion: una de caracteres a numeros y otra de numeros a caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IalZLbvOzf-F",
    "outputId": "fd81ff07-a95d-46f7-942d-6cbffc0c3989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  \"'\" :   3,\n",
      "  ',' :   4,\n",
      "  '-' :   5,\n",
      "  '.' :   6,\n",
      "  ':' :   7,\n",
      "  ';' :   8,\n",
      "  '?' :   9,\n",
      "  'A' :  10,\n",
      "  'B' :  11,\n",
      "  'C' :  12,\n",
      "  'D' :  13,\n",
      "  'E' :  14,\n",
      "  'F' :  15,\n",
      "  'G' :  16,\n",
      "  'H' :  17,\n",
      "  'I' :  18,\n",
      "  'J' :  19,\n",
      "  'K' :  20,\n",
      "  'L' :  21,\n",
      "  'M' :  22,\n",
      "  'N' :  23,\n",
      "  'O' :  24,\n",
      "  'P' :  25,\n",
      "  'Q' :  26,\n",
      "  'R' :  27,\n",
      "  'S' :  28,\n",
      "  'T' :  29,\n",
      "  'U' :  30,\n",
      "  'V' :  31,\n",
      "  'W' :  32,\n",
      "  'X' :  33,\n",
      "  'Y' :  34,\n",
      "  'Z' :  35,\n",
      "  'a' :  36,\n",
      "  'b' :  37,\n",
      "  'c' :  38,\n",
      "  'd' :  39,\n",
      "  'e' :  40,\n",
      "  'f' :  41,\n",
      "  'g' :  42,\n",
      "  'h' :  43,\n",
      "  'i' :  44,\n",
      "  'j' :  45,\n",
      "  'k' :  46,\n",
      "  'l' :  47,\n",
      "  'm' :  48,\n",
      "  'n' :  49,\n",
      "  'o' :  50,\n",
      "  'p' :  51,\n",
      "  'q' :  52,\n",
      "  'r' :  53,\n",
      "  's' :  54,\n",
      "  't' :  55,\n",
      "  'u' :  56,\n",
      "  'v' :  57,\n",
      "  'w' :  58,\n",
      "  'x' :  59,\n",
      "  'y' :  60,\n",
      "  'z' :  61,\n"
     ]
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "for char,_ in zip(char2idx, range(len(vocab))):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una representacion de entero (integer) para cada caracter que podemos ver ejecutando el codigo previo.\n",
    "\n",
    "Con esta funcion inversa a la anterior, podemos pasar el texto a enteros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML-DuUyi_aHy"
   },
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobarlo podemos mostrar los 50 primeros caracteres del texto contenido en el tensor text_as_int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "l1VKcQHcymwb",
    "outputId": "0ef61420-a0eb-4164-f732-37239c69cdf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto: \"That, poor contempt, or claim'd thou slept so fait\"\n",
      "array([29, 43, 36, 55,  4,  1, 51, 50, 50, 53,  1, 38, 50, 49, 55, 40, 48,\n",
      "       51, 55,  4,  1, 50, 53,  1, 38, 47, 36, 44, 48,  3, 39,  1, 55, 43,\n",
      "       50, 56,  1, 54, 47, 40, 51, 55,  1, 54, 50,  1, 41, 36, 44, 55])\n"
     ]
    }
   ],
   "source": [
    "print ('texto: {}'.format(repr(text[:50])))\n",
    "print ('{}'.format(repr(text_as_int[:50])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Preparar los datos para entrenar la RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo prepararemos unas secuencias de caracteres como entrada y salida de un tamaño determinado. En nuestro ejemplo se definio el tamaño de 100 caracteres con la variable seq_lenght.\n",
    "\n",
    "Empezamos dividiendo el texto que tenemos en secuencias de caracteres con las cuales luego construiremos los datos de entrenamiento compuestos por las entradas de seq_lenght caracteres y las salidas correspondientes que cntienen la misma longitud de textom excepto que se desplaza un caracter a la derecha. \n",
    "Por ejemplo, suponiendo un seq_lenght=3 y teniendo como texto un \"Hola\", la secuencia de entrada seria \"Hol\", y la de salida \"ola\".\n",
    "\n",
    "Se utilizara la siguiente funcion, que crea un conjunto de datos con el contenido del tensor text_as_int que contiene el texto, al que podremos aplicar el metodo batch() para dividir este conjunto de datos en secuencias de seq_lenght+1 de indice de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "seq_length = 100\n",
    " \n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que sequences contiene el texto divido en paquetes de 101 caracteres como esperamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "l4hkDU3i7ozi",
    "outputId": "4b0f9234-3e2c-4a97-c253-b271d82b9520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"That, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their defe\"\n",
      "'ated queen,\\nHer flesh broke me and puttance of expedition house,\\nAnd in that same that ever I lament '\n",
      "'this stomach,\\nAnd he, nor Butly and my fury, knowing everything\\nGrew daily ever, his great strength a'\n",
      "\"nd thought\\nThe bright buds of mine own.\\n\\nBIONDELLO:\\nMarry, that it may not pray their patience.'\\n\\nKIN\"\n",
      "'G LEAR:\\nThe instant common maid, as we may less be\\na brave gentleman and joiner: he that finds us wit'\n",
      "\"h wax\\nAnd owe so full of presence and our fooder at our\\nstaves. It is remorsed the bridal's man his g\"\n",
      "'race\\nfor every business in my tongue, but I was thinking\\nthat he contends, he hath respected thee.\\n\\nB'\n",
      "\"IRON:\\nShe left thee on, I'll die to blessed and most reasonable\\nNature in this honour, and her bosom \"\n",
      "'is safe, some\\nothers from his speedy-birth, a bill and as\\nForestem with Richard in your heart\\nBe ques'\n",
      "\"tion'd on, nor that I was enough:\\nWhich of a partier forth the obsers d'punish'd the hate\\nTo my restr\"\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(10):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta secuencia se obtiene el conjunto de datos de training que contenga tanto los datos de entrada (desde la posicion 0 a 99) como los datos de salida (desde la posicion 1 a la 100). Para ello se crea una funcion que realiza esta tarea y se aplica a todas las secuencias usando el metodo map() de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, dataset contiene un conjunto de parejas de secuencias de texto (con la representación numérica de los caracteres), donde el primer componente de la pareja contiene un paquete con una secuencia de 100 caracteres del texto original y la segunda su correspondiente salida, también de 100 caracteres. Podemos comprobarlo visualizándolo por pantalla (por ejemplo mostrando la primera pareja):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "GNbw-iR0ymwj",
    "outputId": "fda91b1a-1845-46dd-915e-490953b99144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"That, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their def\"\n",
      "Target data: \"hat, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their defe\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto del código disponemos de los datos de entrenamiento en el tensor dataseten forma de parejas de secuencias de 100 integers de 32 bits que representan un carácter del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eBu9WZG84i0",
    "outputId": "fe45c87d-9085-463f-bfd1-a4f23d2c2a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print (dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, los datos ya estan preprocesados en el formato que se requiere para ser usados en el entrenamiento de la red neuronal, pero recordemos que en en redes neuronales los datos se agrupan en batches antes de pasarlos al modelo. En nuestro caso hemos decidido un tamaño de batch de 64.\n",
    "\n",
    "En este codigo, para crear los batches de parejas de secuencias se utiliza tf.data que ademas nos permite barajar las secuencias prebiamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2pGotuNzf-S",
    "outputId": "dee41632-e0b6-4102-89c1-c37064974de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print (dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "### Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizara una version minima de RNN para simplificar el ejemplo, que contenga solo una capa de LSTM. En concreto se define una red de solo 3 capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pKJl2dqwHRl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(input_dim=vocab_size, \n",
    "                      output_dim=embedding_dim, \n",
    "                      batch_input_shape=[batch_size, None]))\n",
    "  model.add(LSTM(rnn_units,\n",
    "                 return_sequences=True,\n",
    "                 stateful=True,\n",
    "                 recurrent_initializer='glorot_uniform'))\n",
    "  model.add(Dense(vocab_size))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primer capa es de tipo Word Embedding, que mapea cada caracter de entrada en un vector Embedding. \n",
    "\n",
    "Como argumentos, especificamos el tamaño del vocabulario, especificado con el argumento vocab_size, que indica cuantos vectores Embedding tendra la capa. A continuacion indicamos las dimensiones de estos vectores mediante el argumento embedding_dim. Finalmente se indica el tamaño del batch que usaremos para entrenar.\n",
    "\n",
    "La segunda capa es de tipo LSTM. En la misma indicamos el numero de neuronas recurrentes con el argumento rnn_units.\n",
    "\n",
    "Con return_sequence se indica que queremos predecir el caracter siguiente a todos los caracteres de entrada, no solo el siguiente al ultimo caracter.\n",
    "\n",
    "El argumento stateful indica, explicado de manera simple, el uso de las capacidades de memoria de la red entre baches. Si este argumento esta instanciado a false, indica que con cada nuevo batch se inicializan las memory cells, mientras que si esta en true se esta indicando que para cada batch se mantendran las actualizaciones hechas durante la ejecucion del batch anterior.\n",
    "\n",
    "El ultimo argumento que usamos es recurrent_kernel, donde indicamos como se deben inicializar los pesos de las matrices internas de la red. Usamos la distibucion glorot_uniform, habitual en estos casos.\n",
    "\n",
    "Finalmente la ultima capa es de tipo Dense. Aqui lo importante es el argumento units que nos dice cuantas neuronas tendra la capa y que nos marcara la dimension de la salida. En nuestro caso sera igual al tamaño de nuestro vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "PpHGXDMcZ0Zt",
    "outputId": "086470f9-57f7-4d3d-d446-55f945fed488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           15872     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 62)            63550     \n",
      "=================================================================\n",
      "Total params: 5,326,398\n",
      "Trainable params: 5,326,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, la capa LSTM consta de una gran cantidad de parametros, mas de 5 millones, como era de esperar. \n",
    "\n",
    "Para cada caracter de entrada (transormado a su equivalente numerico), el modelo busco su vector de Embedding correspondiente y luego ejecuta la capa LSTM con este vector como entrada. A la salida de esta capa, aplica la capa Dense para decidir cual es el siguiente caracter.\n",
    "\n",
    "Inspeccionemos las diemnsiones de los tensores para comprender mas a fondo el modelo. Nos fijamos en el primer batch del conjunto de datos de entrenamiento y observamos su forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "C-_70kKAPrPU",
    "outputId": "b71d7b4f-c635-4774-f058-d67067a72eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (64, 100) # (batch_size, sequence_length)\n",
      "Target: (64, 100) # (batch_size, sequence_length)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
    "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en esta red la secuencia de entrada son de batches de 100 caracteres, pero el modelo una vez entrenado puede ser ejecutado con cualquier tamaño de cadena de entrada.\n",
    "\n",
    "Como salida el modelo nos devuelve un tensor con una dimension adicional con la verosimilitud para cada caracter del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udE_6eTldMhB",
    "outputId": "56ea08dd-4b13-45dd-d665-04c6bcc7c58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  (64, 100, 62) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):  \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es elegir uno de los caracteres. Sin entrar en detalle, no se eligira el caracter mas \"probable\" utilizando por ejemplo argmax, puesto que el modelo puede llegar a entrar en un bucle. Lo que se hara es obtener una muestra de la distribucion de salida. Probandolo para el primer ejemplo en el batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "YqFMUQc_UFgM",
    "outputId": "62cd2cbb-80bc-4f6d-9161-6471e8eb4d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 29, 44, 39, 10, 60, 14, 53, 41, 49, 22,  6,  0,  5, 19, 20,  5,\n",
       "       36,  5, 10, 48, 34, 44, 61, 52,  4, 35, 14, 23, 39, 57, 17, 55, 15,\n",
       "       14,  5,  9, 15, 41,  3, 53, 21, 53, 31, 59, 12, 61, 56, 39, 17, 29,\n",
       "        9, 60, 12,  0, 29, 60,  9, 14, 16, 60,  0, 53, 27, 45, 55, 27,  2,\n",
       "       31, 59, 19, 32,  5, 35, 13, 19, 47, 14,  5, 30, 53, 16,  3, 61, 46,\n",
       "       23, 18,  9, 37, 61,  6, 12, 33,  2, 27, 32, 23, 17, 53, 16],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "# Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, el problema puede tratarse como un problema de clasificacion estandar para el que debemos definir la funcion de costo y el optimizador.\n",
    "\n",
    "Para la funcion de costo usaremos la funcion estandar categorical_crossentropy dado que estamos considerando datos categoricos. Y dado que el retorno, como hemos visto, se trata de valores de verosimilitud (no de probabilidades como si hubieramos ya aplicado softmax) se instanciara el argumento from_logits a True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al optimizador, usaremos Adam con sus argumentos por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "Configurar el *checkpoints*\n",
    "\n",
    "Esta es una tecnica de tolerancia de fallos para procesos cuyo tiempo de ejecucion es muy largo. La idea es guardar una instancia del estado del sistema periodicamente para recuperar desde ese punto la ejecucion en caso de fallo del sistema. En nuestro caso, cuando entrenamos modelos Deep Learning, el Checkpoint lo forman basicamente los pesos del modelo. Estos Checkpoints se pueden usar tambien para hacer predicciones tal cual como haremos en este ejemplo.\n",
    "\n",
    "La libreria de Keras proporciona Checkpoints a traves de la API Callbacks. Concretamente usaremos ModelCheckpint para especificar como salvar los Checkpoints a cada epoch durante el entrenamiento, a traves de un argumento en el metodo fit() del modelo.\n",
    "\n",
    "En el código debemos especificar el directorio en el que se guardarán los Checkpoints que salvaremos y el nombre del fichero (que le añadiremos el número de epoch para nuestra comodidad):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    " # directorio\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# nombre fichero\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "*Training*\n",
    "\n",
    "Ahora ya esta todo preparado para empezar a entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UK-hmKjYVoll",
    "outputId": "1ecb5c0b-068a-41e5-9e13-e47a432035cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 57s 4s/step - loss: 2.1178\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 58s 4s/step - loss: 2.0690\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 58s 4s/step - loss: 2.0249\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 59s 4s/step - loss: 1.9799\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 58s 4s/step - loss: 1.9371\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 58s 4s/step - loss: 1.9005\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 58s 4s/step - loss: 1.8595\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.8223\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.7862\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.7471\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.7111\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.6719\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.6319\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 61s 4s/step - loss: 1.5950\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.5583\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.5187\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.4740\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.4315\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 60s 4s/step - loss: 1.3866\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 59s 4s/step - loss: 1.3385\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "### Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el modelo entrenado, podemos pasar a usarlo para generar texto. \n",
    "\n",
    "Para realizar este paso de prediccion simple, vamos a usar un tamaño de batcj de 1. Debido a la forma en que se pasa el estado de la RNN de un instante de tiempo al siguiente, el modelo solo acepta un tamaño de batch fijo una vez construido. Por ello, para poder ejecutar el modelo con un tamaño de bartch diferente, necesitamos reconstruir manualmente el modelo con el metodo build() y restaurar sus pesos desde el ultimo checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz",
    "outputId": "ef8b7f87-a25c-4b48-e662-cc13a2876a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_20'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos pasar a generar texto a partir de una palabra de partida con el siguiente codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "  num_generate = 500\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  text_generated = []\n",
    "\n",
    "\n",
    "  temperature = 0.5\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo empieza con inicializaciones como: definir el numero de caracteres a predecir con la variable num_generate, convertir la palabra inicial (start_string) a su correspondiente representacion numerica y preparar los tensores necesarios.\n",
    "\n",
    "Se usa una variable temperatura para decidir que tan conservador en sus predicciones queremos que se comporte nuestro modelo.\n",
    "\n",
    "Con \"temperaturas altas\" (hasta 1) se permitira mas creativivdad al modelo para generar texto, pero a costa de mas errores (por ej, errores ortograficos, etc). Mientras que con \"temperaturas bajas\" habra menos errores pero el modelo mostrara poca creatividad. \n",
    "\n",
    "A partir de este momento empieza el bucle para generar los caracteres que le hemos indicado (que usa el caracter de entrada la primera vez) y luego sus propias predicciones como entrada a cada iteracion al modelo RNN.\n",
    "\n",
    "Recordemos que estamos en un batch de 1 pero el modelo returno el tensore de batch con las dimensiones que lo habiamos entrenado y por tanto debemos reducir la dimension batch (squeeze).\n",
    "\n",
    "Luego se usa una distribucion categorica para clacular el indice del caracter predicho.\n",
    "\n",
    "Este caracter recien predicho, se usa como proxima entrada al modelo, retroalimentando al mismo para que ahora tenga mas contexto (en lugar de solo una letra). Despues de predecir la siguiente letra, se retroalimenta nuevamente, y asi sucesivamente de manera que va aprendiendo a medida que se obtiene mas contexto de los caracteres predichcos previamente.\n",
    "\n",
    "Ahora que se ha descrito como se ha programado la funcion generate_text() probemos como se comporta el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ktovv0RFhrkn",
    "outputId": "2b110c21-3286-4f43-98fa-7bcda1a9f209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acund father, he hath sonceren shall be stord my partain, and the queen of the raughter of your sendery,\n",
      "There being come to the stake of your day be the bedices and easter shall not his good bear to every death, but and the stares brow the intament of the gone.\n",
      "\n",
      "SARANIO:\n",
      "Here be the cours are here, she is me be stand and acoun a ploine, she dish thing of my bead;\n",
      "And let de the more that the child and lead not love so fair a hink,\n",
      "And be my lord I have a true bedore your grace so make my lord;\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden probar distintos strings iniciales para observar como se comporta el modelo a cada uno de ellos.\n",
    "\n",
    "En resumen, el modelo presentado parece que ha aprendido a generar texto de manera interesanto, teniendo en cuenta el reducido dataset inicial con el que se ha entrenado."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13-Redes-neuronales-recurrentes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
